{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_yYfbCEsHLzQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import cv2 as cv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iZiLp1RFIzp7"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, classes, class_dic, transform = None):\n",
        "        self.annotations = []\n",
        "        self.classes = []\n",
        "        for c in classes:\n",
        "          for file in os.listdir(root+c):\n",
        "            #if file.endswith(\".jpg\"):\n",
        "            self.annotations.append(root+c+'/'+file)\n",
        "            self.classes.append(class_dic[c])\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        image = cv.imread(self.annotations[index],cv.IMREAD_GRAYSCALE)\n",
        "        norm_image = image/255\n",
        "        y = self.classes[index]\n",
        "        y_label = torch.tensor(int(y))\n",
        "\n",
        "        if self.transform:\n",
        "            norm_image = self.transform(image)\n",
        "        return (norm_image, y_label)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzx0B7tcJDI1",
        "outputId": "82d8a345-6899-4018-eb4f-4b24e8a418a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "['checkered_resize', 'dotted_resize', 'solid', 'striped_resize']\n",
            "{'checkered_resize': 0, 'dotted_resize': 1, 'solid': 2, 'striped_resize': 3}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "root='./dataset/'\n",
        "\n",
        "print(device)\n",
        "classes = []\n",
        "class_dic = {}\n",
        "\n",
        "j = 0\n",
        "for file in sorted(os.listdir(root)):\n",
        "    d = os.path.join(root, file)\n",
        "    if os.path.isdir(d):\n",
        "        classes.append(file)\n",
        "        class_dic[file] = j\n",
        "        j=j+1\n",
        "print(classes)\n",
        "print(class_dic)\n",
        "#classes = ['solid','checkered_resize', 'striped_resize', \"dotted_resize\"]\n",
        "#class_dic ={'striped_resize':0,'dotted_resize':1,'checkered_resize':2,'solid':3}\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "batch_size = 4\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7uhc97pZJaRx"
      },
      "outputs": [],
      "source": [
        "from math import *\n",
        "\n",
        "dataset = CustomDataset(root_dir=root, classes=classes, class_dic=class_dic, transform=transforms.ToTensor())\n",
        "length = dataset.__len__()\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [length-floor(length/5), floor(length/5)])\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gUf47mXVKGZD"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5)\n",
        "        self.pool = nn.MaxPool2d(3, 3)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(288, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 288)\n",
        "        x = F.softmax(self.fc1(x),dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvH4fk1IK_Bg",
        "outputId": "f7fe998b-bbe3-46d2-b84b-da397e585ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9565\n"
          ]
        }
      ],
      "source": [
        "model = ConvNet().to(device)\n",
        "print(length)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D7hVyokELCjQ",
        "outputId": "97790218-07e0-4b2a-d9e1-ec7c7f535ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "Epoch [1/5], Step [200/1913], Loss: 1.3640\n",
            "Epoch [1/5], Step [400/1913], Loss: 1.4356\n",
            "Epoch [1/5], Step [600/1913], Loss: 1.3213\n",
            "Epoch [1/5], Step [800/1913], Loss: 1.2340\n",
            "Epoch [1/5], Step [1000/1913], Loss: 1.3694\n",
            "Epoch [1/5], Step [1200/1913], Loss: 1.5276\n",
            "Epoch [1/5], Step [1400/1913], Loss: 1.3263\n",
            "Epoch [1/5], Step [1600/1913], Loss: 1.4492\n",
            "Epoch [1/5], Step [1800/1913], Loss: 1.2126\n",
            "2\n",
            "Epoch [2/5], Step [200/1913], Loss: 1.3082\n",
            "Epoch [2/5], Step [400/1913], Loss: 1.3892\n",
            "Epoch [2/5], Step [600/1913], Loss: 1.0785\n",
            "Epoch [2/5], Step [800/1913], Loss: 0.9921\n",
            "Epoch [2/5], Step [1000/1913], Loss: 1.1530\n",
            "Epoch [2/5], Step [1200/1913], Loss: 1.4621\n",
            "Epoch [2/5], Step [1400/1913], Loss: 1.3256\n",
            "Epoch [2/5], Step [1600/1913], Loss: 1.3945\n",
            "Epoch [2/5], Step [1800/1913], Loss: 1.0284\n",
            "3\n",
            "Epoch [3/5], Step [200/1913], Loss: 1.1017\n",
            "Epoch [3/5], Step [400/1913], Loss: 1.2360\n",
            "Epoch [3/5], Step [600/1913], Loss: 1.0717\n",
            "Epoch [3/5], Step [800/1913], Loss: 1.2153\n",
            "Epoch [3/5], Step [1000/1913], Loss: 1.1576\n",
            "Epoch [3/5], Step [1200/1913], Loss: 1.2559\n",
            "Epoch [3/5], Step [1400/1913], Loss: 0.8136\n",
            "Epoch [3/5], Step [1600/1913], Loss: 1.0133\n",
            "Epoch [3/5], Step [1800/1913], Loss: 0.7836\n",
            "4\n",
            "Epoch [4/5], Step [200/1913], Loss: 1.2092\n",
            "Epoch [4/5], Step [400/1913], Loss: 1.0798\n",
            "Epoch [4/5], Step [600/1913], Loss: 1.4700\n",
            "Epoch [4/5], Step [800/1913], Loss: 1.1875\n",
            "Epoch [4/5], Step [1000/1913], Loss: 0.8399\n",
            "Epoch [4/5], Step [1200/1913], Loss: 0.8114\n",
            "Epoch [4/5], Step [1400/1913], Loss: 1.2409\n",
            "Epoch [4/5], Step [1600/1913], Loss: 1.2358\n",
            "Epoch [4/5], Step [1800/1913], Loss: 1.2435\n",
            "5\n",
            "Epoch [5/5], Step [200/1913], Loss: 1.1788\n",
            "Epoch [5/5], Step [400/1913], Loss: 1.0122\n",
            "Epoch [5/5], Step [600/1913], Loss: 1.6458\n",
            "Epoch [5/5], Step [800/1913], Loss: 1.2436\n",
            "Epoch [5/5], Step [1000/1913], Loss: 0.9706\n",
            "Epoch [5/5], Step [1200/1913], Loss: 1.0178\n",
            "Epoch [5/5], Step [1400/1913], Loss: 0.8770\n",
            "Epoch [5/5], Step [1600/1913], Loss: 0.9687\n",
            "Epoch [5/5], Step [1800/1913], Loss: 1.2423\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "loss_value = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(epoch+1)\n",
        "    running_loss=0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 200 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "    \n",
        "    epoch_loss = running_loss / (length-floor(length/5))\n",
        "    loss_value.append(epoch_loss)\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "P_XvUEWfLg2t",
        "outputId": "b6cf198b-3e78-440b-8917-f4775ccd61f7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(loss_value)\n",
        "PATH = root + '/cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ4I1BBqLq06",
        "outputId": "2f8a5f30-863e-4070-c73d-106075aa4433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1912\n",
            "tensor([0, 3, 3, 2])\n",
            "torch.Size([4, 1, 150, 150])\n",
            "3\n",
            "tensor([1, 3, 3, 3])\n",
            "torch.Size([4, 1, 150, 150])\n",
            "3\n",
            "tensor([0, 2, 2, 0])\n",
            "torch.Size([4, 1, 150, 150])\n",
            "3\n",
            "tensor([1, 0, 3, 0])\n",
            "torch.Size([4, 1, 150, 150])\n",
            "3\n",
            "tensor([3, 1, 0, 1])\n",
            "tensor([1])\n",
            "Accuracy of the network: 71.77208572922112 %\n",
            "Accuracy of the network 66.32996632996633\n",
            "checkered_resize\n",
            "Accuracy of the network 82.97029702970298\n",
            "dotted_resize\n",
            "Accuracy of the network 54.03508771929825\n",
            "solid\n",
            "Accuracy of the network 76.74858223062382\n",
            "striped_resize\n",
            "torch.Size([1, 1, 150, 150])\n",
            "tensor([[1.0039e-01, 8.9961e-01, 5.2679e-38, 7.2137e-10]])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(len(classes))]\n",
        "    n_class_samples = [0 for i in range(len(classes))]\n",
        "    j=0\n",
        "    length2 = len(test_loader)\n",
        "    for images, labels in test_loader:\n",
        "        if j % 100 == 0:\n",
        "          print(i)\n",
        "          print(labels)\n",
        "        j=j+1\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        if j % 100 == 0:\n",
        "          print(images.size())\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            try:\n",
        "              label = labels[i]\n",
        "            except:\n",
        "              print(labels)\n",
        "              break\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of the network {acc}')\n",
        "        print(classes[i])\n",
        "    image = cv.imread('./dataset/dotted_resize/dotted6.jpg',cv.IMREAD_GRAYSCALE)\n",
        "    image = image\n",
        "    transform = transforms.ToTensor()\n",
        "    image = transform(image)\n",
        "    image = image[None, :]\n",
        "    print(image.size())\n",
        "    output = model(image)\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting Frontend ==> MIL Ops:  98%|█████████▊| 64/65 [00:00<00:00, 717.54 ops/s]\n",
            "Running MIL Common passes:   0%|          | 0/34 [00:00<?, ? passes/s]/Users/alexandrebarbier/Library/Python/3.8/lib/python/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '78', of the source model, has been renamed to 'var_78' in the Core ML model.\n",
            "  warnings.warn(msg.format(var.name, new_name))\n",
            "Running MIL Common passes: 100%|██████████| 34/34 [00:00<00:00, 469.56 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 9/9 [00:00<00:00, 511.24 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 52/52 [00:00<00:00, 13214.41 ops/s]\n"
          ]
        }
      ],
      "source": [
        "import coremltools as ct\n",
        "\n",
        "model = ConvNet()\n",
        "model.load_state_dict(torch.load(root + 'cnn.pth'))\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.rand(1, 1, 150, 150)\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "\n",
        "class_labels = [0,1,2,3]\n",
        "classifier_config = ct.ClassifierConfig(class_labels)\n",
        "\n",
        "_input =ct.ImageType(name=\"input_1\",shape=(1,1,150,150),scale = 1./255)\n",
        "\n",
        "mlmodel = ct.convert(traced_model, inputs=[_input], classifier_config=classifier_config)\n",
        "\n",
        "mlmodel.save(\"tesstmodel.mlmodel\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "test-pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
