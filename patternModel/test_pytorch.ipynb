{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_yYfbCEsHLzQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import cv2 as cv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iZiLp1RFIzp7"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, classes, class_dic, transform = None):\n",
        "        self.annotations = []\n",
        "        self.classes = []\n",
        "        for c in classes:\n",
        "          for file in os.listdir(root+c):\n",
        "            #if file.endswith(\".jpg\"):\n",
        "            self.annotations.append(root+c+'/'+file)\n",
        "            self.classes.append(class_dic[c])\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        image = cv.imread(self.annotations[index],cv.IMREAD_GRAYSCALE)\n",
        "        norm_image = image/255\n",
        "        y = self.classes[index]\n",
        "        y_label = torch.tensor(int(y))\n",
        "\n",
        "        if self.transform:\n",
        "            norm_image = self.transform(image)\n",
        "        return (norm_image, y_label)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzx0B7tcJDI1",
        "outputId": "82d8a345-6899-4018-eb4f-4b24e8a418a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "['checkered_resize', 'dotted_resize', 'solid', 'striped_resize']\n",
            "{'checkered_resize': 0, 'dotted_resize': 1, 'solid': 2, 'striped_resize': 3}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "root='./dataset/'\n",
        "\n",
        "print(device)\n",
        "classes = []\n",
        "class_dic = {}\n",
        "\n",
        "j = 0\n",
        "for file in sorted(os.listdir(root)):\n",
        "    d = os.path.join(root, file)\n",
        "    if os.path.isdir(d):\n",
        "        classes.append(file)\n",
        "        class_dic[file] = j\n",
        "        j=j+1\n",
        "print(classes)\n",
        "print(class_dic)\n",
        "#classes = ['solid','checkered_resize', 'striped_resize', \"dotted_resize\"]\n",
        "#class_dic ={'striped_resize':0,'dotted_resize':1,'checkered_resize':2,'solid':3}\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 4\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7uhc97pZJaRx"
      },
      "outputs": [],
      "source": [
        "from math import *\n",
        "\n",
        "dataset = CustomDataset(root_dir=root, classes=classes, class_dic=class_dic, transform=transforms.ToTensor())\n",
        "length = dataset.__len__()\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [length-floor(length/5), floor(length/5)])\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gUf47mXVKGZD"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5)\n",
        "        self.pool = nn.MaxPool2d(3, 3)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(288, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 288)\n",
        "        x = F.softmax(self.fc1(x),dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvH4fk1IK_Bg",
        "outputId": "f7fe998b-bbe3-46d2-b84b-da397e585ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9565\n"
          ]
        }
      ],
      "source": [
        "model = ConvNet().to(device)\n",
        "print(length)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D7hVyokELCjQ",
        "outputId": "97790218-07e0-4b2a-d9e1-ec7c7f535ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "Epoch [1/10], Step [200/1913], Loss: 1.3434\n",
            "Epoch [1/10], Step [400/1913], Loss: 1.3746\n",
            "Epoch [1/10], Step [600/1913], Loss: 1.3393\n",
            "Epoch [1/10], Step [800/1913], Loss: 1.4816\n",
            "Epoch [1/10], Step [1000/1913], Loss: 1.0682\n",
            "Epoch [1/10], Step [1200/1913], Loss: 1.1277\n",
            "Epoch [1/10], Step [1400/1913], Loss: 1.4288\n",
            "Epoch [1/10], Step [1600/1913], Loss: 1.1391\n",
            "Epoch [1/10], Step [1800/1913], Loss: 1.1875\n",
            "2\n",
            "Epoch [2/10], Step [200/1913], Loss: 1.0852\n",
            "Epoch [2/10], Step [400/1913], Loss: 1.3357\n",
            "Epoch [2/10], Step [600/1913], Loss: 1.6290\n",
            "Epoch [2/10], Step [800/1913], Loss: 1.3858\n",
            "Epoch [2/10], Step [1000/1913], Loss: 0.7940\n",
            "Epoch [2/10], Step [1200/1913], Loss: 0.8037\n",
            "Epoch [2/10], Step [1400/1913], Loss: 1.2753\n",
            "Epoch [2/10], Step [1600/1913], Loss: 1.4673\n",
            "Epoch [2/10], Step [1800/1913], Loss: 1.0822\n",
            "3\n",
            "Epoch [3/10], Step [200/1913], Loss: 1.4690\n",
            "Epoch [3/10], Step [400/1913], Loss: 1.4817\n",
            "Epoch [3/10], Step [600/1913], Loss: 1.2257\n",
            "Epoch [3/10], Step [800/1913], Loss: 0.7776\n",
            "Epoch [3/10], Step [1000/1913], Loss: 0.7491\n",
            "Epoch [3/10], Step [1200/1913], Loss: 0.9078\n",
            "Epoch [3/10], Step [1400/1913], Loss: 1.2183\n",
            "Epoch [3/10], Step [1600/1913], Loss: 0.8192\n",
            "Epoch [3/10], Step [1800/1913], Loss: 1.0063\n",
            "4\n",
            "Epoch [4/10], Step [200/1913], Loss: 0.8565\n",
            "Epoch [4/10], Step [400/1913], Loss: 1.0071\n",
            "Epoch [4/10], Step [600/1913], Loss: 1.0113\n",
            "Epoch [4/10], Step [800/1913], Loss: 1.4117\n",
            "Epoch [4/10], Step [1000/1913], Loss: 0.7442\n",
            "Epoch [4/10], Step [1200/1913], Loss: 1.2382\n",
            "Epoch [4/10], Step [1400/1913], Loss: 0.9844\n",
            "Epoch [4/10], Step [1600/1913], Loss: 0.7531\n",
            "Epoch [4/10], Step [1800/1913], Loss: 1.0248\n",
            "5\n",
            "Epoch [5/10], Step [200/1913], Loss: 0.7454\n",
            "Epoch [5/10], Step [400/1913], Loss: 0.8190\n",
            "Epoch [5/10], Step [600/1913], Loss: 0.9926\n",
            "Epoch [5/10], Step [800/1913], Loss: 1.1904\n",
            "Epoch [5/10], Step [1000/1913], Loss: 0.7533\n",
            "Epoch [5/10], Step [1200/1913], Loss: 0.8005\n",
            "Epoch [5/10], Step [1400/1913], Loss: 1.1545\n",
            "Epoch [5/10], Step [1600/1913], Loss: 0.7908\n",
            "Epoch [5/10], Step [1800/1913], Loss: 0.9683\n",
            "6\n",
            "Epoch [6/10], Step [200/1913], Loss: 0.7755\n",
            "Epoch [6/10], Step [400/1913], Loss: 1.2268\n",
            "Epoch [6/10], Step [600/1913], Loss: 0.9969\n",
            "Epoch [6/10], Step [800/1913], Loss: 0.7821\n",
            "Epoch [6/10], Step [1000/1913], Loss: 1.1628\n",
            "Epoch [6/10], Step [1200/1913], Loss: 0.7588\n",
            "Epoch [6/10], Step [1400/1913], Loss: 0.9967\n",
            "Epoch [6/10], Step [1600/1913], Loss: 1.0728\n",
            "Epoch [6/10], Step [1800/1913], Loss: 0.7776\n",
            "7\n",
            "Epoch [7/10], Step [200/1913], Loss: 1.0748\n",
            "Epoch [7/10], Step [400/1913], Loss: 1.4680\n",
            "Epoch [7/10], Step [600/1913], Loss: 0.8281\n",
            "Epoch [7/10], Step [800/1913], Loss: 0.7443\n",
            "Epoch [7/10], Step [1000/1913], Loss: 0.7444\n",
            "Epoch [7/10], Step [1200/1913], Loss: 1.4886\n",
            "Epoch [7/10], Step [1400/1913], Loss: 1.0106\n",
            "Epoch [7/10], Step [1600/1913], Loss: 1.0231\n",
            "Epoch [7/10], Step [1800/1913], Loss: 0.9807\n",
            "8\n",
            "Epoch [8/10], Step [200/1913], Loss: 0.9896\n",
            "Epoch [8/10], Step [400/1913], Loss: 1.5041\n",
            "Epoch [8/10], Step [600/1913], Loss: 0.7437\n",
            "Epoch [8/10], Step [800/1913], Loss: 0.9938\n",
            "Epoch [8/10], Step [1000/1913], Loss: 0.9933\n",
            "Epoch [8/10], Step [1200/1913], Loss: 1.1714\n",
            "Epoch [8/10], Step [1400/1913], Loss: 0.9909\n",
            "Epoch [8/10], Step [1600/1913], Loss: 0.8981\n",
            "Epoch [8/10], Step [1800/1913], Loss: 0.9919\n",
            "9\n",
            "Epoch [9/10], Step [200/1913], Loss: 0.7927\n",
            "Epoch [9/10], Step [400/1913], Loss: 0.7449\n",
            "Epoch [9/10], Step [600/1913], Loss: 0.9937\n",
            "Epoch [9/10], Step [800/1913], Loss: 0.9668\n",
            "Epoch [9/10], Step [1000/1913], Loss: 1.0342\n",
            "Epoch [9/10], Step [1200/1913], Loss: 0.9497\n",
            "Epoch [9/10], Step [1400/1913], Loss: 0.9897\n",
            "Epoch [9/10], Step [1600/1913], Loss: 0.7457\n",
            "Epoch [9/10], Step [1800/1913], Loss: 0.9937\n",
            "10\n",
            "Epoch [10/10], Step [200/1913], Loss: 1.2244\n",
            "Epoch [10/10], Step [400/1913], Loss: 0.9027\n",
            "Epoch [10/10], Step [600/1913], Loss: 0.7666\n",
            "Epoch [10/10], Step [800/1913], Loss: 1.2392\n",
            "Epoch [10/10], Step [1000/1913], Loss: 0.9783\n",
            "Epoch [10/10], Step [1200/1913], Loss: 0.9466\n",
            "Epoch [10/10], Step [1400/1913], Loss: 0.9346\n",
            "Epoch [10/10], Step [1600/1913], Loss: 1.1202\n",
            "Epoch [10/10], Step [1800/1913], Loss: 0.9599\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "loss_value = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(epoch+1)\n",
        "    running_loss=0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 200 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "    \n",
        "    epoch_loss = running_loss / (length-floor(length/5))\n",
        "    loss_value.append(epoch_loss)\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "P_XvUEWfLg2t",
        "outputId": "b6cf198b-3e78-440b-8917-f4775ccd61f7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(loss_value)\n",
        "PATH = root + '/cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ4I1BBqLq06",
        "outputId": "2f8a5f30-863e-4070-c73d-106075aa4433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "tensor([3, 3, 3, 0])\n",
            "torch.Size([4, 1, 150, 150])\n",
            "3\n",
            "tensor([0, 0, 1, 3])\n",
            "torch.Size([4, 1, 150, 150])\n",
            "3\n",
            "tensor([0, 0, 3, 3])\n",
            "torch.Size([4, 1, 150, 150])\n",
            "3\n",
            "tensor([0, 0, 1, 3])\n",
            "torch.Size([4, 1, 150, 150])\n",
            "3\n",
            "tensor([0, 3, 3, 3])\n",
            "tensor([0])\n",
            "Accuracy of the network: 74.54260324098274 %\n",
            "Accuracy of the network 79.64912280701755\n",
            "checkered_resize\n",
            "Accuracy of the network 80.50682261208577\n",
            "dotted_resize\n",
            "Accuracy of the network 52.059925093632955\n",
            "solid\n",
            "Accuracy of the network 74.60035523978685\n",
            "striped_resize\n",
            "torch.Size([1, 1, 150, 150])\n",
            "tensor([[9.2558e-16, 1.0000e+00, 0.0000e+00, 3.2399e-26]])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(len(classes))]\n",
        "    n_class_samples = [0 for i in range(len(classes))]\n",
        "    j=0\n",
        "    length2 = len(test_loader)\n",
        "    for images, labels in test_loader:\n",
        "        if j % 100 == 0:\n",
        "          print(i)\n",
        "          print(labels)\n",
        "        j=j+1\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        if j % 100 == 0:\n",
        "          print(images.size())\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            try:\n",
        "              label = labels[i]\n",
        "            except:\n",
        "              print(labels)\n",
        "              break\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of the network {acc}')\n",
        "        print(classes[i])\n",
        "    image = cv.imread('./dataset/dotted_resize/dotted6.jpg',cv.IMREAD_GRAYSCALE)\n",
        "    image = image\n",
        "    transform = transforms.ToTensor()\n",
        "    image = transform(image)\n",
        "    image = image[None, :]\n",
        "    print(image.size())\n",
        "    output = model(image)\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting Frontend ==> MIL Ops:  98%|█████████▊| 64/65 [00:00<00:00, 134.19 ops/s]\n",
            "Running MIL Common passes:   0%|          | 0/34 [00:00<?, ? passes/s]/Users/alexandrebarbier/Library/Python/3.8/lib/python/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '78', of the source model, has been renamed to 'var_78' in the Core ML model.\n",
            "  warnings.warn(msg.format(var.name, new_name))\n",
            "Running MIL Common passes: 100%|██████████| 34/34 [00:00<00:00, 598.78 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 9/9 [00:00<00:00, 446.69 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 52/52 [00:00<00:00, 1900.87 ops/s]\n"
          ]
        }
      ],
      "source": [
        "import coremltools as ct\n",
        "\n",
        "model = ConvNet()\n",
        "model.load_state_dict(torch.load('cnn.pth'))\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.rand(1, 1, 150, 150)\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "\n",
        "class_labels = [0,1,2,3]\n",
        "classifier_config = ct.ClassifierConfig(class_labels)\n",
        "\n",
        "_input =ct.ImageType(name=\"input_1\",shape=(1,1,150,150),scale = 1./255)\n",
        "\n",
        "mlmodel = ct.convert(traced_model, inputs=[_input], classifier_config=classifier_config)\n",
        "\n",
        "mlmodel.save(\"tesstmodel.mlmodel\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "test-pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
