{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_yYfbCEsHLzQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import cv2 as cv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iZiLp1RFIzp7"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, classes, class_dic, transform = None):\n",
        "        self.annotations = []\n",
        "        self.classes = []\n",
        "        for c in classes:\n",
        "          for file in os.listdir(root+c):\n",
        "            #if file.endswith(\".jpg\"):\n",
        "            self.annotations.append(root+c+'/'+file)\n",
        "            self.classes.append(class_dic[c])\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        image = cv.imread(self.annotations[index],cv.IMREAD_GRAYSCALE)\n",
        "        norm_image = image/255\n",
        "        y = self.classes[index]\n",
        "        y_label = torch.tensor(int(y))\n",
        "\n",
        "        if self.transform:\n",
        "            norm_image = self.transform(image)\n",
        "        return (norm_image, y_label)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzx0B7tcJDI1",
        "outputId": "82d8a345-6899-4018-eb4f-4b24e8a418a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "['checkered_resize', 'solid', 'dotted_resize', 'striped_resize']\n",
            "{'checkered_resize': 0, 'solid': 1, 'dotted_resize': 2, 'striped_resize': 3}\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "root='./dataset/'\n",
        "\n",
        "print(device)\n",
        "classes = []\n",
        "class_dic = {}\n",
        "\n",
        "j = 0\n",
        "for file in sorted(os.listdir(root)):\n",
        "    d = os.path.join(root, file)\n",
        "    if os.path.isdir(d):\n",
        "        classes.append(file)\n",
        "        class_dic[file] = j\n",
        "        j=j+1\n",
        "print(classes)\n",
        "print(class_dic)\n",
        "#classes = ['solid','checkered_resize', 'striped_resize', \"dotted_resize\"]\n",
        "#class_dic ={'striped_resize':0,'dotted_resize':1,'checkered_resize':2,'solid':3}\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 4\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7uhc97pZJaRx"
      },
      "outputs": [],
      "source": [
        "from math import *\n",
        "\n",
        "dataset = CustomDataset(root_dir=root, classes=classes, class_dic=class_dic, transform=transforms.ToTensor())\n",
        "length = dataset.__len__()\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [length-floor(length/5), floor(length/5)])\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gUf47mXVKGZD"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5)\n",
        "        self.pool = nn.MaxPool2d(3, 3)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(288, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 288)\n",
        "        x = F.softmax(self.fc1(x),dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvH4fk1IK_Bg",
        "outputId": "f7fe998b-bbe3-46d2-b84b-da397e585ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9565\n"
          ]
        }
      ],
      "source": [
        "model = ConvNet().to(device)\n",
        "print(length)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D7hVyokELCjQ",
        "outputId": "97790218-07e0-4b2a-d9e1-ec7c7f535ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'to'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/alexandrebarbier/Downloads/data/test_pytorch.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandrebarbier/Downloads/data/test_pytorch.ipynb#ch0000006?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandrebarbier/Downloads/data/test_pytorch.ipynb#ch0000006?line=6'>7</a>\u001b[0m     \u001b[39m# origin shape: [4, 3, 32, 32] = 4, 3, 1024\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandrebarbier/Downloads/data/test_pytorch.ipynb#ch0000006?line=7'>8</a>\u001b[0m     \u001b[39m# input_layer: 3 input channels, 6 output channels, 5 kernel size\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexandrebarbier/Downloads/data/test_pytorch.ipynb#ch0000006?line=8'>9</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexandrebarbier/Downloads/data/test_pytorch.ipynb#ch0000006?line=9'>10</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexandrebarbier/Downloads/data/test_pytorch.ipynb#ch0000006?line=11'>12</a>\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexandrebarbier/Downloads/data/test_pytorch.ipynb#ch0000006?line=12'>13</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "loss_value = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(epoch+1)\n",
        "    running_loss=0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 200 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "    \n",
        "    epoch_loss = running_loss / (length-floor(length/5))\n",
        "    loss_value.append(epoch_loss)\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "P_XvUEWfLg2t",
        "outputId": "b6cf198b-3e78-440b-8917-f4775ccd61f7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(loss_value)\n",
        "PATH = root + '/cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ4I1BBqLq06",
        "outputId": "2f8a5f30-863e-4070-c73d-106075aa4433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "tensor([0, 2, 1, 2])\n",
            "3\n",
            "tensor([1, 1, 0, 1])\n",
            "3\n",
            "tensor([1, 3, 1, 1])\n",
            "3\n",
            "tensor([0, 3, 1, 2])\n",
            "3\n",
            "tensor([2, 0, 0, 2])\n",
            "tensor([0])\n",
            "Accuracy of the network: 63.30371144798745 %\n",
            "Accuracy of the network 88.19188191881919\n",
            "solid\n",
            "Accuracy of the network 45.05494505494506\n",
            "checkered_resize\n",
            "Accuracy of the network 61.20401337792642\n",
            "striped_resize\n",
            "Accuracy of the network 53.30396475770925\n",
            "dotted_resize\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(len(classes))]\n",
        "    n_class_samples = [0 for i in range(len(classes))]\n",
        "    j=0\n",
        "    length2 = len(test_loader)\n",
        "    for images, labels in test_loader:\n",
        "        if j % 100 == 0:\n",
        "          print(i)\n",
        "          print(labels)\n",
        "        j=j+1\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            try:\n",
        "              label = labels[i]\n",
        "            except:\n",
        "              print(labels)\n",
        "              break\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of the network {acc}')\n",
        "        print(classes[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting Frontend ==> MIL Ops:  98%|█████████▊| 64/65 [00:00<00:00, 134.19 ops/s]\n",
            "Running MIL Common passes:   0%|          | 0/34 [00:00<?, ? passes/s]/Users/alexandrebarbier/Library/Python/3.8/lib/python/site-packages/coremltools/converters/mil/mil/passes/name_sanitization_utils.py:129: UserWarning: Output, '78', of the source model, has been renamed to 'var_78' in the Core ML model.\n",
            "  warnings.warn(msg.format(var.name, new_name))\n",
            "Running MIL Common passes: 100%|██████████| 34/34 [00:00<00:00, 598.78 passes/s]\n",
            "Running MIL Clean up passes: 100%|██████████| 9/9 [00:00<00:00, 446.69 passes/s]\n",
            "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 52/52 [00:00<00:00, 1900.87 ops/s]\n"
          ]
        }
      ],
      "source": [
        "import coremltools as ct\n",
        "\n",
        "model = ConvNet()\n",
        "model.load_state_dict(torch.load('cnn.pth'))\n",
        "\n",
        "# Create dummy input\n",
        "dummy_input = torch.rand(1, 1, 150, 150)\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "\n",
        "class_labels = [0,1,2,3]\n",
        "classifier_config = ct.ClassifierConfig(class_labels)\n",
        "\n",
        "_input =ct.ImageType(name=\"input_1\",shape=(1,1,150,150),scale = 1./255)\n",
        "\n",
        "mlmodel = ct.convert(traced_model, inputs=[_input], classifier_config=classifier_config)\n",
        "\n",
        "mlmodel.save(\"tesstmodel.mlmodel\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "test-pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
